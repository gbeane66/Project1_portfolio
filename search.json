[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "",
    "text": "The purpose of this project is to use machine learning to determine whether water is safe for human consumption based on a range of different metrics."
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "",
    "text": "The purpose of this project is to use machine learning to determine whether water is safe for human consumption based on a range of different metrics."
  },
  {
    "objectID": "index.html#step-1.0-eda",
    "href": "index.html#step-1.0-eda",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "ðŸ’§ Step 1.0 | EDA",
    "text": "ðŸ’§ Step 1.0 | EDA\nTo begin with letâ€™s load the data as a polars DataFrame. Why use the Polars package and not the Pandas package? Speed. The Polars package is much faster than Pandas and, while this is not really a factor in this project owing to the small size of the dataset, it might be useful to familiarise myself with any differences in polars now. In any case,\n\nStep 1.1 | Import libraries\n\n\nCode\n# Import packages for data manipulation\nimport polars as pl\nimport numpy as np\n\n# Import packages for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\n# Import packages for data preprocessing\nfrom sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n\n\n# Import packages for data modeling\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n\n# Import three methods for ML\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n\n\n\nStep 1.2 | Load Data\n\n\nCode\ndata = pl.read_csv('water_potability.csv')\n\n\n\n\nStep 1.3 | View Data\n\n\nCode\ndata.head()\n\n\n\nshape: (5, 10)\n\n\n\nph\nHardness\nSolids\nChloramines\nSulfate\nConductivity\nOrganic_carbon\nTrihalomethanes\nTurbidity\nPotability\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\n\n\n\n\nnull\n204.890455\n20791.318981\n7.300212\n368.516441\n564.308654\n10.379783\n86.99097\n2.963135\n0\n\n\n3.71608\n129.422921\n18630.057858\n6.635246\nnull\n592.885359\n15.180013\n56.329076\n4.500656\n0\n\n\n8.099124\n224.236259\n19909.541732\n9.275884\nnull\n418.606213\n16.868637\n66.420093\n3.055934\n0\n\n\n8.316766\n214.373394\n22018.417441\n8.059332\n356.886136\n363.266516\n18.436524\n100.341674\n4.628771\n0\n\n\n9.092223\n181.101509\n17978.986339\n6.5466\n310.135738\n398.410813\n11.558279\n31.997993\n4.075075\n0\n\n\n\n\n\n\nOK, so there are 10 fields in the DataFrame (pH,Hardness,Solids, Chloramines,Sulfate,Conductivity,Organic Carbon,Trihalomethanes,Turbidity and Potability). Of these, Potability is the target variable (the one we are trying to predict) and it is binary - it can either be potable (1) or not-potable (0). The other 9 fields are all float values.\nLetâ€™s now have an overview of the fields in terms of nulls and various other statistics.\n\n\nCode\ndata.describe()\n\n\n\nshape: (9, 11)\n\n\n\nstatistic\nph\nHardness\nSolids\nChloramines\nSulfate\nConductivity\nOrganic_carbon\nTrihalomethanes\nTurbidity\nPotability\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n2785.0\n3276.0\n3276.0\n3276.0\n2495.0\n3276.0\n3276.0\n3114.0\n3276.0\n3276.0\n\n\n\"null_count\"\n491.0\n0.0\n0.0\n0.0\n781.0\n0.0\n0.0\n162.0\n0.0\n0.0\n\n\n\"mean\"\n7.080795\n196.369496\n22014.092526\n7.122277\n333.775777\n426.205111\n14.28497\n66.396293\n3.966786\n0.39011\n\n\n\"std\"\n1.59432\n32.879761\n8768.570828\n1.583085\n41.41684\n80.824064\n3.308162\n16.175008\n0.780382\n0.487849\n\n\n\"min\"\n0.0\n47.432\n320.942611\n0.352\n129.0\n181.483754\n2.2\n0.738\n1.45\n0.0\n\n\n\"25%\"\n6.093092\n176.853696\n15668.273618\n6.127804\n307.704474\n365.739122\n12.065963\n55.835966\n3.43974\n0.0\n\n\n\"50%\"\n7.036752\n196.982379\n20933.51275\n7.130437\n333.073546\n421.890083\n14.219303\n66.623944\n3.955091\n0.0\n\n\n\"75%\"\n8.062066\n216.665319\n27331.361962\n8.114731\n359.951766\n481.771934\n16.557177\n77.339918\n4.500208\n1.0\n\n\n\"max\"\n14.0\n323.124\n61227.196008\n13.127\n481.030642\n753.34262\n28.3\n124.0\n6.739\n1.0\n\n\n\n\n\n\nBased on the statistical overview of the fields, the things that jump out at me are:\n\npH, Sulfate and Trihalomethanes all have significant numbers of null values. These will have to be either dropped or filled.\nOther fields look reasonable, though the range of pH (from 0.0 to 14.0) while physically valid is outrageous. This means some of the tested water sources were extremely acidic (pH 0.0) or extremely basic (pH 14.0).\n\nAre any of the fields obviously correlated with each other? We can do a quick check of this using a simple pairplot.\n\n\nCode\nsns.pairplot(data.to_pandas(), hue='Potability', corner=True, palette='Greens')\nplt.show()\n\n\n\n\n\n\n\n\n\nAnother way of showing the same thing\n\n\nCode\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(data.to_pandas().corr(),annot=True,cmap='Greens',ax=ax)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThere does not seem to be any correlation between any of the fields - no obvious linear relationship is apparrent in the correlation plots."
  },
  {
    "objectID": "index.html#step-2.0-pre-process-data",
    "href": "index.html#step-2.0-pre-process-data",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "Step 2.0 | Pre-process data",
    "text": "Step 2.0 | Pre-process data\nNow that we know a little bit more about our data, letâ€™s deal with some problems that we identified in Step 1. Namely, what do we do about the null values?\n\n\nCode\nprint('Percentage(%) of nulls in each column: \\n')\n\nprint(data.to_pandas().isna().sum()/len(data)*100)\n\n\nPercentage(%) of nulls in each column: \n\nph                 14.987790\nHardness            0.000000\nSolids              0.000000\nChloramines         0.000000\nSulfate            23.840049\nConductivity        0.000000\nOrganic_carbon      0.000000\nTrihalomethanes     4.945055\nTurbidity           0.000000\nPotability          0.000000\ndtype: float64"
  }
]
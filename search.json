[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "",
    "text": "The purpose of this project is to use machine learning to determine whether water is safe for human consumption based on a range of different metrics."
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "",
    "text": "The purpose of this project is to use machine learning to determine whether water is safe for human consumption based on a range of different metrics."
  },
  {
    "objectID": "index.html#step-1.0-eda",
    "href": "index.html#step-1.0-eda",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "ðŸ’§ Step 1.0 | EDA",
    "text": "ðŸ’§ Step 1.0 | EDA\nTo begin with letâ€™s load the data as a polars DataFrame. Why use the Polars package and not the Pandas package? Speed. The Polars package is much faster than Pandas and, while this is not really a factor in this project owing to the small size of the dataset, it might be useful to familiarise myself with any differences in polars now. In any case,\n\nStep 1.1 | Import libraries\n\n\nCode\n# Import packages for data manipulation\nimport polars as pl\nimport numpy as np\n\n# Import packages for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom termcolor import colored\n\n# Import packages for data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n\n\n# Import packages for data modeling\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n\n# Import four methods for ML\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n\n\n\nStep 1.2 | Load Data\n\n\nCode\ndata = pl.read_csv('water_potability.csv')\n\n\n\n\nStep 1.3 | View Data\n\n\nCode\ndata.head()\n\n\n\nshape: (5, 10)\n\n\n\nph\nHardness\nSolids\nChloramines\nSulfate\nConductivity\nOrganic_carbon\nTrihalomethanes\nTurbidity\nPotability\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\n\n\n\n\nnull\n204.890455\n20791.318981\n7.300212\n368.516441\n564.308654\n10.379783\n86.99097\n2.963135\n0\n\n\n3.71608\n129.422921\n18630.057858\n6.635246\nnull\n592.885359\n15.180013\n56.329076\n4.500656\n0\n\n\n8.099124\n224.236259\n19909.541732\n9.275884\nnull\n418.606213\n16.868637\n66.420093\n3.055934\n0\n\n\n8.316766\n214.373394\n22018.417441\n8.059332\n356.886136\n363.266516\n18.436524\n100.341674\n4.628771\n0\n\n\n9.092223\n181.101509\n17978.986339\n6.5466\n310.135738\n398.410813\n11.558279\n31.997993\n4.075075\n0\n\n\n\n\n\n\nOK, so there are 10 fields in the DataFrame (pH,Hardness,Solids, Chloramines,Sulfate,Conductivity,Organic Carbon,Trihalomethanes,Turbidity and Potability). Of these, Potability is the target variable (the one we are trying to predict) and it is binary - it can either be potable (1) or not-potable (0). The other 9 fields are all float values.\nLetâ€™s now have an overview of the fields in terms of nulls and various other statistics.\n\n\nCode\ndata.describe()\n\n\n\nshape: (9, 11)\n\n\n\nstatistic\nph\nHardness\nSolids\nChloramines\nSulfate\nConductivity\nOrganic_carbon\nTrihalomethanes\nTurbidity\nPotability\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n2785.0\n3276.0\n3276.0\n3276.0\n2495.0\n3276.0\n3276.0\n3114.0\n3276.0\n3276.0\n\n\n\"null_count\"\n491.0\n0.0\n0.0\n0.0\n781.0\n0.0\n0.0\n162.0\n0.0\n0.0\n\n\n\"mean\"\n7.080795\n196.369496\n22014.092526\n7.122277\n333.775777\n426.205111\n14.28497\n66.396293\n3.966786\n0.39011\n\n\n\"std\"\n1.59432\n32.879761\n8768.570828\n1.583085\n41.41684\n80.824064\n3.308162\n16.175008\n0.780382\n0.487849\n\n\n\"min\"\n0.0\n47.432\n320.942611\n0.352\n129.0\n181.483754\n2.2\n0.738\n1.45\n0.0\n\n\n\"25%\"\n6.093092\n176.853696\n15668.273618\n6.127804\n307.704474\n365.739122\n12.065963\n55.835966\n3.43974\n0.0\n\n\n\"50%\"\n7.036752\n196.982379\n20933.51275\n7.130437\n333.073546\n421.890083\n14.219303\n66.623944\n3.955091\n0.0\n\n\n\"75%\"\n8.062066\n216.665319\n27331.361962\n8.114731\n359.951766\n481.771934\n16.557177\n77.339918\n4.500208\n1.0\n\n\n\"max\"\n14.0\n323.124\n61227.196008\n13.127\n481.030642\n753.34262\n28.3\n124.0\n6.739\n1.0\n\n\n\n\n\n\nBased on the statistical overview of the fields, the things that jump out at me are:\n\npH, Sulfate and Trihalomethanes all have significant numbers of null values. These will have to be either dropped or filled.\nOther fields look reasonable, though the range of pH (from 0.0 to 14.0) while physically valid is outrageous. This means some of the tested water sources were extremely acidic (pH 0.0) or extremely basic (pH 14.0).\n\nAre any of the fields obviously correlated with each other? We can do a quick check of this using a simple pairplot.\n\n\nCode\nsns.pairplot(data.to_pandas(), hue='Potability', corner=True, palette='Greens')\nplt.show()\n\n\n\n\n\nAnother way of showing the same thing\n\n\nCode\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(data.to_pandas().corr(),annot=True,cmap='Greens',ax=ax)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\nThere does not seem to be any correlation between any of the fields - no obvious linear relationship is apparrent in the correlation plots."
  },
  {
    "objectID": "index.html#step-2.0-pre-process-data",
    "href": "index.html#step-2.0-pre-process-data",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "Step 2.0 | Pre-process data",
    "text": "Step 2.0 | Pre-process data\nNow that we know a little bit more about our data, letâ€™s deal with some problems that we identified in Step 1. Namely, what do we do about the null values?\nIf we look at the percentage of each column that has null counts\n\n\nCode\nprint('Percentage(%) of nulls in each column: \\n')\n\nprint(data.to_pandas().isna().sum()/len(data)*100)\n\n\nPercentage(%) of nulls in each column: \n\nph                 14.987790\nHardness            0.000000\nSolids              0.000000\nChloramines         0.000000\nSulfate            23.840049\nConductivity        0.000000\nOrganic_carbon      0.000000\nTrihalomethanes     4.945055\nTurbidity           0.000000\nPotability          0.000000\ndtype: float64\n\n\nwe see that for fields pH and Sulfate the fraction that are null is significant. We could simply drop each row that has a null value, but this is quite wasteful and might introduce unwanted artifacts to the modelling.\nInstead, we can replace any null value with the median value for that field. Before we do that though, it might be good to just check that there is no difference in the median value for the pH,Sulfate and Trihalomethane fields for potable vs non-potable water.\n\n\nCode\nimport pandas as pd\ndata2 = data.to_pandas()\nprint('Median for Non-Potable water')\ndata2[data2.Potability==0][['ph','Sulfate','Trihalomethanes']].median()\n\n\nMedian for Non-Potable water\n\n\nph                   7.035456\nSulfate            333.389426\nTrihalomethanes     66.542198\ndtype: float64\n\n\n\n\nCode\nprint('Median for Potable water')\ndata2[data2.Potability==1][['ph','Sulfate','Trihalomethanes']].median()\n\n\nMedian for Potable water\n\n\nph                   7.036752\nSulfate            331.838167\nTrihalomethanes     66.678214\ndtype: float64\n\n\nThe median value for the field which contain null counts doesnâ€™t seem to be different depending on whether the water is potable or not.\nRight, letâ€™s replace the nulls with the median values\n\n\nCode\nfor field in ['ph','Sulfate','Trihalomethanes']:\n  data2[field] = data2[field].fillna(value=data2[field].median())\n\n\n\n\nCode\nnulls = data2.isna().sum().sum()\nprint(nulls)\n\n\n0\n\n\nIt worked! All missing values have been replaced with the field mean. We can now move on to normalisation.\n\nStep 2.1 | Normalisation\nThe target field is Potability. We need to move the predictor fields to a variable X and the target to y.\n\n\nCode\nX = data2.drop(columns='Potability')\ny = data2['Potability'] \n\n\nNow we want to scale with MinMaxScaler so values in the preidctor fields are mapped to the range [0,1].\n\n\nCode\nscaler = MinMaxScaler(feature_range=(0,1))\ndf = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\ndf.describe().loc[['min','mean','std','max']].T.style.background_gradient(axis=1)\n\n\n\n\n\n\n\nÂ \nmin\nmean\nstd\nmax\n\n\n\n\nph\n0.000000\n0.505300\n0.105003\n1.000000\n\n\nHardness\n0.000000\n0.540231\n0.119263\n1.000000\n\n\nSolids\n0.000000\n0.356173\n0.143968\n1.000000\n\n\nChloramines\n0.000000\n0.529963\n0.123921\n1.000000\n\n\nSulfate\n0.000000\n0.581223\n0.102672\n1.000000\n\n\nConductivity\n0.000000\n0.427940\n0.141336\n1.000000\n\n\nOrganic_carbon\n0.000000\n0.463026\n0.126750\n1.000000\n\n\nTrihalomethanes\n0.000000\n0.532763\n0.127939\n1.000000\n\n\nTurbidity\n0.000000\n0.475853\n0.147548\n1.000000\n\n\n\n\n\nGreat! It seems that all of the predictor fields have been normalised."
  },
  {
    "objectID": "index.html#step-3.0-modelling",
    "href": "index.html#step-3.0-modelling",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "Step 3.0 | Modelling",
    "text": "Step 3.0 | Modelling\nFirst letâ€™s split our data into a test and a train set.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n\n\nLetâ€™s creat a function to compare the results of modelling.\n\n\nCode\ndef plot_result(y_pred) :\n    '''\n    1) Plot a Confusion Matrix\n    2) Plot a Classification Report for each model\n    '''\n    fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n    fig.tight_layout()\n    #Left axis: Confusion Matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    ax[0]=sns.heatmap(cm, cmap='Blues', annot=True, fmt='', linewidths=0.5, ax=ax[0])\n    ax[0].set_xlabel('Prediced labels', fontsize=18)\n    ax[0].set_ylabel('True labels', fontsize=18)\n    ax[0].set_title('Confusion Matrix', fontsize=25)\n    ax[0].xaxis.set_ticklabels(['0', '1'])\n    ax[0].yaxis.set_ticklabels(['0', '1'])\n\n    # Right axis: Classification Report\n    cr = pd.DataFrame(metrics.classification_report(y_test, y_pred, digits=3, output_dict=True)).T\n    cr.drop(columns='support', inplace=True)\n    ax[1] = sns.heatmap(cr, cmap='Blues', annot=True, fmt='0.3f', linewidths=0.5, ax=ax[1])\n    ax[1].xaxis.tick_top()\n    ax[1].set_title('Classification Report', fontsize=25)\n    plt.show()\n\n\n\nStep 3.1 | Logistic Regression\nLetâ€™s start with a very simple model.\n\n\nCode\n# a dictionary to define parameters to test in algorithm\nparameters = {\n    'C' : [0.001, 1, 1000],\n    'class_weight' : ['balanced', None],\n    'solver' : ['liblinear', 'sag'],\n    'penalty' : ['l2'],\n    'verbose': [0],\n    'max_iter': [100000]\n}\n\nlr = LogisticRegression()\ngrid = GridSearchCV(estimator=lr, param_grid=parameters, verbose=0, cv=5)\n\nlr_cv = grid.fit(X_train, y_train);\n\n\nprint(colored('Tuned hyper parameters :\\n{}'.format(lr_cv.best_params_), 'blue'))\n\n\nTuned hyper parameters :\n{'C': 1, 'class_weight': None, 'max_iter': 100000, 'penalty': 'l2', 'solver': 'liblinear', 'verbose': 0}\n\n\n\n\nCode\nlr = LogisticRegression(**lr_cv.best_params_).fit(X_train, y_train)\n\ny_pred_lr = lr.predict(X_test)\n\nlr_score = round(lr.score(X_test, y_test), 3)\nprint(colored('LogisticRegression Score : {}'.format(lr_score), 'green'))\n\n\nLogisticRegression Score : 0.627\n\n\n\n\nCode\nplot_result(y_pred_lr)\n\n\n\n\n\nSeems OK, but letâ€™s try some other models and see if we can do better!\n\n\nStep 3.2 | XGBoost\nOne model that often forms quite well is XGBoost, letâ€™s give it a try.\n\n\nCode\n# Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic',random_state=0)\n\n# Create a dictionary of hyperparameters to tune\n\nparameters = {\"max_depth\":[5],\n            \"min_child_weight\":[1],\n             \"learning_rate\":[0.2,0.5],\n             \"n_estimators\":[5],\n             \"subsample\":[0.6],\n             \"colsample_bytree\":[0.6]\n            }\n\n# Define a dictionary of scoring metrics to capture\nxgb_scoring = [\"accuracy\",\"precision\",\"recall\",\"f1\"]\n\n# Instantiate the GridSearchCV object\nxgb_cv = GridSearchCV(xgb, parameters, scoring=xgb_scoring, cv=5, refit='f1')\n\nxgb_cv.fit(X_train, y_train)\n\nxgb_cv.best_params_\n\n\n{'colsample_bytree': 0.6,\n 'learning_rate': 0.5,\n 'max_depth': 5,\n 'min_child_weight': 1,\n 'n_estimators': 5,\n 'subsample': 0.6}\n\n\n\n\nCode\nxgb = XGBClassifier(**xgb_cv.best_params_).fit(X_train, y_train)\n\ny_pred_xgb = xgb.predict(X_test)\n\nxgb_score = round(xgb.score(X_test, y_test), 3)\nprint(colored('XGB Score : {}'.format(xgb_score), 'green'))\n\n\nXGB Score : 0.652\n\n\n\n\nCode\nplot_result(y_pred_xgb)\n\n\n\n\n\nBetter than LogisticRegression but I still think we can do better.\n\n\nStep 3.3 | SVC\n\n\nCode\nparameters = {\n    'C' : [1e-6,0.0001,1,10,100,300,1000],\n    'gamma' : [1e-6,0.0001,1,10,100],\n}\n\n\n\nsvc = SVC()\nsvc_cv = GridSearchCV(estimator=svc, param_grid=parameters, cv=30).fit(X_train, y_train)\n\n\n\nprint('Tuned hyper parameters : ', svc_cv.best_params_)\nprint('accuracy : ', svc_cv.best_score_)\n\n\nTuned hyper parameters :  {'C': 100, 'gamma': 1e-06}\naccuracy :  0.6140978753047719\n\n\n\n\nCode\nsvc = SVC(**svc_cv.best_params_).fit(X_train, y_train)\n\ny_pred_svc = svc.predict(X_test)\n\nsvc_score = round(svc.score(X_test, y_test), 3)\nprint(colored('SVC Score : {}'.format(svc_score), 'green'))\n\n\nSVC Score : 0.625\n\n\n\n\nCode\nplot_result(y_pred_svc)\n\n\n\n\n\nThe worst yet, letâ€™s try an old favourite RandomForest.\n\n\nStep 3.4 | RandomForest\n\n\nCode\nparameters = {\n    'n_estimators' : [1000],\n    'criterion' : ['log_loss'],\n    'max_features' : ['sqrt'],\n    'n_jobs' : [-1]\n}\n\nrf = RandomForestClassifier()\nrf_cv = GridSearchCV(estimator=rf, param_grid=parameters, cv=20).fit(X_train, y_train)\nprint(colored('Tuned hyper parameters :\\n{}'.format(rf_cv.best_params_), 'blue'))\n\n\nTuned hyper parameters :\n{'criterion': 'log_loss', 'max_features': 'sqrt', 'n_estimators': 1000, 'n_jobs': -1}\n\n\n\n\nCode\nrf = RandomForestClassifier(**rf_cv.best_params_).fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nrf_score = round(rf.score(X_test, y_test), 3)\nprint(colored('RandomForestClassifier Score : {}'.format(rf_score), 'green'))\n\n\nRandomForestClassifier Score : 0.68\n\n\n\n\nCode\nplot_result(y_pred_rf)\n\n\n\n\n\n\n\nCode\nimportances = rf.feature_importances_\npd.Series(importances, index=X.columns).sort_values()\n\n\nTurbidity          0.097799\nOrganic_carbon     0.098967\nTrihalomethanes    0.101153\nConductivity       0.101974\nSolids             0.113494\nChloramines        0.115837\nSulfate            0.122122\nHardness           0.122936\nph                 0.125718\ndtype: float64\n\n\nThe best yet - it seems like the winner is the RandomForest model with score of 0.694."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Portfolio Project 1: Predicting water potability",
    "section": "Conclusion",
    "text": "Conclusion\nLetâ€™s review the different models.\n\n\nCode\nresult = pd.DataFrame({\n    'Algorithm' : ['RandomForestClassifier', 'XGBoostClassifier','LogisticRegression', 'SVCClassifier'],\n    'Score' : [rf_score, xgb_score, lr_score,  svc_score]\n})\n\n\nresult.style.background_gradient()\n\n\n\n\n\n\n\nÂ \nAlgorithm\nScore\n\n\n\n\n0\nRandomForestClassifier\n0.680000\n\n\n1\nXGBoostClassifier\n0.652000\n\n\n2\nLogisticRegression\n0.627000\n\n\n3\nSVCClassifier\n0.625000\n\n\n\n\n\nSo there you have it - sometimes it doesnâ€™t require the most complicated model. The RandomForest model works surprisingly well."
  }
]
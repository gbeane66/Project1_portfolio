{
  "hash": "ed6a952f0d0345b0afa1dd98f801f7c0",
  "result": {
    "markdown": "---\ntitle: \"Portfolio Project 1: Predicting water potability\"\nexecute: \n  echo: true\n\nformat:\n  html:\n    page-layout: full\n    theme:\n      light: cosmo\n      dark: [cosmo, theme_dark_custom.scss]\n    code-link: true\n    code-fold: true\nengine: python3\n---\n\n## Goal\nThe purpose of this project is to use machine learning to determine whether water is safe for human consumption based on a range of different metrics.\n\n## ðŸ’§ Step 1.0 | EDA\n\nTo begin with let's load the data as a polars DataFrame. Why use the Polars package and not the Pandas package? Speed. The Polars package is much faster than Pandas and, while this is not really a factor in this project owing to the small size of the dataset, it might be useful to familiarise myself with any differences in polars now. In any case,\n\n### Step 1.1 | Import libraries\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import packages for data manipulation\nimport polars as pl\nimport numpy as np\n\n# Import packages for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom termcolor import colored\n\n# Import packages for data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n\n\n# Import packages for data modeling\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n\n# Import four methods for ML\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n```\n:::\n\n\n### Step 1.2 | Load Data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pl.read_csv('water_potability.csv')\n```\n:::\n\n\n### Step 1.3 | View Data\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ph</th><th>Hardness</th><th>Solids</th><th>Chloramines</th><th>Sulfate</th><th>Conductivity</th><th>Organic_carbon</th><th>Trihalomethanes</th><th>Turbidity</th><th>Potability</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>null</td><td>204.890455</td><td>20791.318981</td><td>7.300212</td><td>368.516441</td><td>564.308654</td><td>10.379783</td><td>86.99097</td><td>2.963135</td><td>0</td></tr><tr><td>3.71608</td><td>129.422921</td><td>18630.057858</td><td>6.635246</td><td>null</td><td>592.885359</td><td>15.180013</td><td>56.329076</td><td>4.500656</td><td>0</td></tr><tr><td>8.099124</td><td>224.236259</td><td>19909.541732</td><td>9.275884</td><td>null</td><td>418.606213</td><td>16.868637</td><td>66.420093</td><td>3.055934</td><td>0</td></tr><tr><td>8.316766</td><td>214.373394</td><td>22018.417441</td><td>8.059332</td><td>356.886136</td><td>363.266516</td><td>18.436524</td><td>100.341674</td><td>4.628771</td><td>0</td></tr><tr><td>9.092223</td><td>181.101509</td><td>17978.986339</td><td>6.5466</td><td>310.135738</td><td>398.410813</td><td>11.558279</td><td>31.997993</td><td>4.075075</td><td>0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nOK, so there are 10 fields in the DataFrame (`pH`,`Hardness`,`Solids`, `Chloramines`,`Sulfate`,`Conductivity`,`Organic Carbon`,`Trihalomethanes`,`Turbidity` and `Potability`). Of these, `Potability` is the target variable (the one we are trying to predict) and it is *binary* - it can either be **potable** (1) or **not-potable** (0). The other 9 fields are all float values. \n\nLet's now have an overview of the fields in terms of `nulls` and various other statistics.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndata.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (9, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>ph</th><th>Hardness</th><th>Solids</th><th>Chloramines</th><th>Sulfate</th><th>Conductivity</th><th>Organic_carbon</th><th>Trihalomethanes</th><th>Turbidity</th><th>Potability</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>2785.0</td><td>3276.0</td><td>3276.0</td><td>3276.0</td><td>2495.0</td><td>3276.0</td><td>3276.0</td><td>3114.0</td><td>3276.0</td><td>3276.0</td></tr><tr><td>&quot;null_count&quot;</td><td>491.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>781.0</td><td>0.0</td><td>0.0</td><td>162.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>7.080795</td><td>196.369496</td><td>22014.092526</td><td>7.122277</td><td>333.775777</td><td>426.205111</td><td>14.28497</td><td>66.396293</td><td>3.966786</td><td>0.39011</td></tr><tr><td>&quot;std&quot;</td><td>1.59432</td><td>32.879761</td><td>8768.570828</td><td>1.583085</td><td>41.41684</td><td>80.824064</td><td>3.308162</td><td>16.175008</td><td>0.780382</td><td>0.487849</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>47.432</td><td>320.942611</td><td>0.352</td><td>129.0</td><td>181.483754</td><td>2.2</td><td>0.738</td><td>1.45</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>6.093092</td><td>176.853696</td><td>15668.273618</td><td>6.127804</td><td>307.704474</td><td>365.739122</td><td>12.065963</td><td>55.835966</td><td>3.43974</td><td>0.0</td></tr><tr><td>&quot;50%&quot;</td><td>7.036752</td><td>196.982379</td><td>20933.51275</td><td>7.130437</td><td>333.073546</td><td>421.890083</td><td>14.219303</td><td>66.623944</td><td>3.955091</td><td>0.0</td></tr><tr><td>&quot;75%&quot;</td><td>8.062066</td><td>216.665319</td><td>27331.361962</td><td>8.114731</td><td>359.951766</td><td>481.771934</td><td>16.557177</td><td>77.339918</td><td>4.500208</td><td>1.0</td></tr><tr><td>&quot;max&quot;</td><td>14.0</td><td>323.124</td><td>61227.196008</td><td>13.127</td><td>481.030642</td><td>753.34262</td><td>28.3</td><td>124.0</td><td>6.739</td><td>1.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nBased on the statistical overview of the fields, the things that jump out at me are:\n\n1) `pH`, `Sulfate` and `Trihalomethanes` all have significant numbers of `null` values. These will have to be either dropped or filled.\n2) Other fields look reasonable, though the range of pH (from 0.0 to 14.0) while physically valid is outrageous. This means some of the tested water sources were extremely acidic (pH 0.0) or extremely basic (pH 14.0).\n\nAre any of the fields obviously correlated with each other? We can do a quick check of this using a simple pairplot.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsns.pairplot(data.to_pandas(), hue='Potability', corner=True, palette='Greens')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nAnother way of showing the same thing\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(data.to_pandas().corr(),annot=True,cmap='Greens',ax=ax)\nplt.title(\"Correlation Matrix\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nThere does not seem to be any correlation between any of the fields - no obvious linear relationship is apparrent in the correlation plots.\n\n## Step 2.0 | Pre-process data \n\nNow that we know a little bit more about our data, let's deal with some problems that we identified in Step 1. Namely, what do we do about the null values?\n\nIf we look at the percentage of each column that has null counts\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nprint('Percentage(%) of nulls in each column: \\n')\n\nprint(data.to_pandas().isna().sum()/len(data)*100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPercentage(%) of nulls in each column: \n\nph                 14.987790\nHardness            0.000000\nSolids              0.000000\nChloramines         0.000000\nSulfate            23.840049\nConductivity        0.000000\nOrganic_carbon      0.000000\nTrihalomethanes     4.945055\nTurbidity           0.000000\nPotability          0.000000\ndtype: float64\n```\n:::\n:::\n\n\nwe see that for fields `pH` and `Sulfate` the fraction that are null is significant. We could simply drop each row that has a null value, but this is quite wasteful and might introduce unwanted artifacts to the modelling.\n\nInstead, we can replace any null value with the median value for that field. Before we do that though, it might be good to just check that there is no difference in the median value for the `pH`,`Sulfate` and `Trihalomethane` fields for potable vs non-potable water.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\ndata2 = data.to_pandas()\nprint('Median for Non-Potable water')\ndata2[data2.Potability==0][['ph','Sulfate','Trihalomethanes']].median()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian for Non-Potable water\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nph                   7.035456\nSulfate            333.389426\nTrihalomethanes     66.542198\ndtype: float64\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nprint('Median for Potable water')\ndata2[data2.Potability==1][['ph','Sulfate','Trihalomethanes']].median()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian for Potable water\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nph                   7.036752\nSulfate            331.838167\nTrihalomethanes     66.678214\ndtype: float64\n```\n:::\n:::\n\n\nThe median value for the field which contain null counts doesn't seem to be different depending on whether the water is potable or not.\n\nRight, let's replace the nulls with the median values\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfor field in ['ph','Sulfate','Trihalomethanes']:\n  data2[field] = data2[field].fillna(value=data2[field].median())\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnulls = data2.isna().sum().sum()\nprint(nulls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n```\n:::\n:::\n\n\nIt worked! All missing values have been replaced with the field mean. We can now move on to normalisation.\n\n### Step 2.1 | Normalisation\n\nThe target field is `Potability`. We need to move the predictor fields to a variable `X` and the target to `y`.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX = data2.drop(columns='Potability')\ny = data2['Potability'] \n```\n:::\n\n\nNow we want to scale with `MinMaxScaler` so values in the preidctor fields are mapped to the range **[0,1]**.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nscaler = MinMaxScaler(feature_range=(0,1))\ndf = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\ndf.describe().loc[['min','mean','std','max']].T.style.background_gradient(axis=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<style type=\"text/css\">\n#T_fb48d_row0_col0, #T_fb48d_row1_col0, #T_fb48d_row2_col0, #T_fb48d_row3_col0, #T_fb48d_row4_col0, #T_fb48d_row5_col0, #T_fb48d_row6_col0, #T_fb48d_row7_col0, #T_fb48d_row8_col0 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n#T_fb48d_row0_col1 {\n  background-color: #71a8ce;\n  color: #f1f1f1;\n}\n#T_fb48d_row0_col2, #T_fb48d_row4_col2 {\n  background-color: #f0eaf4;\n  color: #000000;\n}\n#T_fb48d_row0_col3, #T_fb48d_row1_col3, #T_fb48d_row2_col3, #T_fb48d_row3_col3, #T_fb48d_row4_col3, #T_fb48d_row5_col3, #T_fb48d_row6_col3, #T_fb48d_row7_col3, #T_fb48d_row8_col3 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_fb48d_row1_col1 {\n  background-color: #60a1ca;\n  color: #f1f1f1;\n}\n#T_fb48d_row1_col2 {\n  background-color: #ede8f3;\n  color: #000000;\n}\n#T_fb48d_row2_col1 {\n  background-color: #acc0dd;\n  color: #000000;\n}\n#T_fb48d_row2_col2, #T_fb48d_row5_col2 {\n  background-color: #e8e4f0;\n  color: #000000;\n}\n#T_fb48d_row3_col1 {\n  background-color: #65a3cb;\n  color: #f1f1f1;\n}\n#T_fb48d_row3_col2 {\n  background-color: #ede7f2;\n  color: #000000;\n}\n#T_fb48d_row4_col1 {\n  background-color: #4c99c5;\n  color: #f1f1f1;\n}\n#T_fb48d_row5_col1 {\n  background-color: #91b5d6;\n  color: #000000;\n}\n#T_fb48d_row6_col1 {\n  background-color: #83afd3;\n  color: #f1f1f1;\n}\n#T_fb48d_row6_col2, #T_fb48d_row7_col2 {\n  background-color: #ece7f2;\n  color: #000000;\n}\n#T_fb48d_row7_col1 {\n  background-color: #63a2cb;\n  color: #f1f1f1;\n}\n#T_fb48d_row8_col1 {\n  background-color: #7eadd1;\n  color: #f1f1f1;\n}\n#T_fb48d_row8_col2 {\n  background-color: #e7e3f0;\n  color: #000000;\n}\n</style>\n<table id=\"T_fb48d\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_fb48d_level0_col0\" class=\"col_heading level0 col0\" >min</th>\n      <th id=\"T_fb48d_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n      <th id=\"T_fb48d_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n      <th id=\"T_fb48d_level0_col3\" class=\"col_heading level0 col3\" >max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_fb48d_level0_row0\" class=\"row_heading level0 row0\" >ph</th>\n      <td id=\"T_fb48d_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row0_col1\" class=\"data row0 col1\" >0.505300</td>\n      <td id=\"T_fb48d_row0_col2\" class=\"data row0 col2\" >0.105003</td>\n      <td id=\"T_fb48d_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row1\" class=\"row_heading level0 row1\" >Hardness</th>\n      <td id=\"T_fb48d_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row1_col1\" class=\"data row1 col1\" >0.540231</td>\n      <td id=\"T_fb48d_row1_col2\" class=\"data row1 col2\" >0.119263</td>\n      <td id=\"T_fb48d_row1_col3\" class=\"data row1 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row2\" class=\"row_heading level0 row2\" >Solids</th>\n      <td id=\"T_fb48d_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row2_col1\" class=\"data row2 col1\" >0.356173</td>\n      <td id=\"T_fb48d_row2_col2\" class=\"data row2 col2\" >0.143968</td>\n      <td id=\"T_fb48d_row2_col3\" class=\"data row2 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row3\" class=\"row_heading level0 row3\" >Chloramines</th>\n      <td id=\"T_fb48d_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row3_col1\" class=\"data row3 col1\" >0.529963</td>\n      <td id=\"T_fb48d_row3_col2\" class=\"data row3 col2\" >0.123921</td>\n      <td id=\"T_fb48d_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row4\" class=\"row_heading level0 row4\" >Sulfate</th>\n      <td id=\"T_fb48d_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row4_col1\" class=\"data row4 col1\" >0.581223</td>\n      <td id=\"T_fb48d_row4_col2\" class=\"data row4 col2\" >0.102672</td>\n      <td id=\"T_fb48d_row4_col3\" class=\"data row4 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row5\" class=\"row_heading level0 row5\" >Conductivity</th>\n      <td id=\"T_fb48d_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row5_col1\" class=\"data row5 col1\" >0.427940</td>\n      <td id=\"T_fb48d_row5_col2\" class=\"data row5 col2\" >0.141336</td>\n      <td id=\"T_fb48d_row5_col3\" class=\"data row5 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row6\" class=\"row_heading level0 row6\" >Organic_carbon</th>\n      <td id=\"T_fb48d_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row6_col1\" class=\"data row6 col1\" >0.463026</td>\n      <td id=\"T_fb48d_row6_col2\" class=\"data row6 col2\" >0.126750</td>\n      <td id=\"T_fb48d_row6_col3\" class=\"data row6 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row7\" class=\"row_heading level0 row7\" >Trihalomethanes</th>\n      <td id=\"T_fb48d_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row7_col1\" class=\"data row7 col1\" >0.532763</td>\n      <td id=\"T_fb48d_row7_col2\" class=\"data row7 col2\" >0.127939</td>\n      <td id=\"T_fb48d_row7_col3\" class=\"data row7 col3\" >1.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_fb48d_level0_row8\" class=\"row_heading level0 row8\" >Turbidity</th>\n      <td id=\"T_fb48d_row8_col0\" class=\"data row8 col0\" >0.000000</td>\n      <td id=\"T_fb48d_row8_col1\" class=\"data row8 col1\" >0.475853</td>\n      <td id=\"T_fb48d_row8_col2\" class=\"data row8 col2\" >0.147548</td>\n      <td id=\"T_fb48d_row8_col3\" class=\"data row8 col3\" >1.000000</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nGreat! It seems that all of the predictor fields have been normalised.\n\n## Step 4.0 | Modelling\n\nFirst let's split our data into a test and a train set.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n```\n:::\n\n\nLet's creat a function to compare the results of modelling.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef plot_result(y_pred) :\n    '''\n    1) Plot a Confusion Matrix\n    2) Plot a Classification Report for each model\n    '''\n    fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n    fig.tight_layout()\n    #Left axis: Confusion Matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    ax[0]=sns.heatmap(cm, cmap='Blues', annot=True, fmt='', linewidths=0.5, ax=ax[0])\n    ax[0].set_xlabel('Prediced labels', fontsize=18)\n    ax[0].set_ylabel('True labels', fontsize=18)\n    ax[0].set_title('Confusion Matrix', fontsize=25)\n    ax[0].xaxis.set_ticklabels(['0', '1'])\n    ax[0].yaxis.set_ticklabels(['0', '1'])\n\n    # Right axis: Classification Report\n    cr = pd.DataFrame(metrics.classification_report(y_test, y_pred, digits=3, output_dict=True)).T\n    cr.drop(columns='support', inplace=True)\n    ax[1] = sns.heatmap(cr, cmap='Blues', annot=True, fmt='0.3f', linewidths=0.5, ax=ax[1])\n    ax[1].xaxis.tick_top()\n    ax[1].set_title('Classification Report', fontsize=25)\n    plt.show()\n```\n:::\n\n\n### Step 4.1 | Logistic Regression\n\nLet's start with a very simple model.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# a dictionary to define parameters to test in algorithm\nparameters = {\n    'C' : [0.001, 1, 1000],\n    'class_weight' : ['balanced', None],\n    'solver' : ['liblinear', 'sag'],\n    'penalty' : ['l2'],\n    'verbose': [0],\n    'max_iter': [100000]\n}\n\nlr = LogisticRegression()\ngrid = GridSearchCV(estimator=lr, param_grid=parameters, verbose=0, cv=5)\n\nlr_cv = grid.fit(X_train, y_train);\n\n\nprint(colored('Tuned hyper parameters :\\n{}'.format(lr_cv.best_params_), 'blue'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTuned hyper parameters :\n{'C': 1, 'class_weight': None, 'max_iter': 100000, 'penalty': 'l2', 'solver': 'liblinear', 'verbose': 0}\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nlr = LogisticRegression(**lr_cv.best_params_).fit(X_train, y_train)\n\ny_pred_lr = lr.predict(X_test)\n\nlr_score = round(lr.score(X_test, y_test), 3)\nprint(colored('LogisticRegression Score : {}'.format(lr_score), 'green'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogisticRegression Score : 0.627\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplot_result(y_pred_lr)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nSeems OK, but let's try some other models and see if we can do better!\n\n### Step 4.2 | XGBoost\nOne model that often forms quite well is `XGBoost`, let's give it a try.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective='binary:logistic',random_state=0)\n\n# Create a dictionary of hyperparameters to tune\n\nparameters = {\"max_depth\":[5],\n            \"min_child_weight\":[1],\n             \"learning_rate\":[0.2,0.5],\n             \"n_estimators\":[5],\n             \"subsample\":[0.6],\n             \"colsample_bytree\":[0.6]\n            }\n\n# Define a dictionary of scoring metrics to capture\nxgb_scoring = [\"accuracy\",\"precision\",\"recall\",\"f1\"]\n\n# Instantiate the GridSearchCV object\nxgb_cv = GridSearchCV(xgb, parameters, scoring=xgb_scoring, cv=5, refit='f1')\n\nxgb_cv.fit(X_train, y_train)\n\nxgb_cv.best_params_\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n{'colsample_bytree': 0.6,\n 'learning_rate': 0.5,\n 'max_depth': 5,\n 'min_child_weight': 1,\n 'n_estimators': 5,\n 'subsample': 0.6}\n```\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nxgb = XGBClassifier(**xgb_cv.best_params_).fit(X_train, y_train)\n\ny_pred_xgb = xgb.predict(X_test)\n\nxgb_score = round(xgb.score(X_test, y_test), 3)\nprint(colored('XGB Score : {}'.format(xgb_score), 'green'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXGB Score : 0.652\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nplot_result(y_pred_xgb)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\nBetter than `LogisticRegression` but I still think we can do better.\n\n### Step 4.3 | SVC\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nparameters = {\n    'C' : [1e-6,0.0001,1,10,100,300,1000],\n    'gamma' : [1e-6,0.0001,1,10,100],\n}\n\n\n\nsvc = SVC()\nsvc_cv = GridSearchCV(estimator=svc, param_grid=parameters, cv=30).fit(X_train, y_train)\n\n\n\nprint('Tuned hyper parameters : ', svc_cv.best_params_)\nprint('accuracy : ', svc_cv.best_score_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTuned hyper parameters :  {'C': 100, 'gamma': 1e-06}\naccuracy :  0.6140978753047719\n```\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nsvc = SVC(**svc_cv.best_params_).fit(X_train, y_train)\n\ny_pred_svc = svc.predict(X_test)\n\nsvc_score = round(svc.score(X_test, y_test), 3)\nprint(colored('SVC Score : {}'.format(svc_score), 'green'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSVC Score : 0.625\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nplot_result(y_pred_svc)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){}\n:::\n:::\n\n\nThe worst yet, let's try an old favourite `RandomForest`.\n\n### Step 4.4 | RandomForest\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nparameters = {\n    'n_estimators' : [1000],\n    'criterion' : ['log_loss'],\n    'max_features' : ['sqrt'],\n    'n_jobs' : [-1]\n}\n\nrf = RandomForestClassifier()\nrf_cv = GridSearchCV(estimator=rf, param_grid=parameters, cv=20).fit(X_train, y_train)\nprint(colored('Tuned hyper parameters :\\n{}'.format(rf_cv.best_params_), 'blue'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTuned hyper parameters :\n{'criterion': 'log_loss', 'max_features': 'sqrt', 'n_estimators': 1000, 'n_jobs': -1}\n```\n:::\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nrf = RandomForestClassifier(**rf_cv.best_params_).fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nrf_score = round(rf.score(X_test, y_test), 3)\nprint(colored('RandomForestClassifier Score : {}'.format(rf_score), 'green'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandomForestClassifier Score : 0.688\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nplot_result(y_pred_rf)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nimportances = rf.feature_importances_\npd.Series(importances, index=X.columns).sort_values()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\nTurbidity          0.098297\nOrganic_carbon     0.098773\nConductivity       0.100825\nTrihalomethanes    0.100877\nSolids             0.113704\nChloramines        0.116733\nSulfate            0.122485\nHardness           0.122587\nph                 0.125719\ndtype: float64\n```\n:::\n:::\n\n\nThe best yet - it seems like the winner is the `RandomForest` model with score of **0.694**.\n\n## Conclusion\n\nLet's review the different models.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nresult = pd.DataFrame({\n    'Algorithm' : ['RandomForestClassifier', 'XGBoostClassifier','LogisticRegression', 'SVCClassifier'],\n    'Score' : [rf_score, xgb_score, lr_score,  svc_score]\n})\n\n\nresult.style.background_gradient()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<style type=\"text/css\">\n#T_01463_row0_col1 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_01463_row1_col1 {\n  background-color: #91b5d6;\n  color: #000000;\n}\n#T_01463_row2_col1 {\n  background-color: #faf3f9;\n  color: #000000;\n}\n#T_01463_row3_col1 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n</style>\n<table id=\"T_01463\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_01463_level0_col0\" class=\"col_heading level0 col0\" >Algorithm</th>\n      <th id=\"T_01463_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_01463_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_01463_row0_col0\" class=\"data row0 col0\" >RandomForestClassifier</td>\n      <td id=\"T_01463_row0_col1\" class=\"data row0 col1\" >0.688000</td>\n    </tr>\n    <tr>\n      <th id=\"T_01463_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_01463_row1_col0\" class=\"data row1 col0\" >XGBoostClassifier</td>\n      <td id=\"T_01463_row1_col1\" class=\"data row1 col1\" >0.652000</td>\n    </tr>\n    <tr>\n      <th id=\"T_01463_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_01463_row2_col0\" class=\"data row2 col0\" >LogisticRegression</td>\n      <td id=\"T_01463_row2_col1\" class=\"data row2 col1\" >0.627000</td>\n    </tr>\n    <tr>\n      <th id=\"T_01463_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_01463_row3_col0\" class=\"data row3 col0\" >SVCClassifier</td>\n      <td id=\"T_01463_row3_col1\" class=\"data row3 col1\" >0.625000</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nSo there you have it - sometimes it doesn't require the most complicated model. The `RandomForest` model works surprisingly well.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}